{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20a4d65ee77906a86bc39bc4046a2a36",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use a cloud GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = \"XXX.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\"\n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72b2403e87a33f87371b150984248355",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "\n",
    "assert (\n",
    "    python_version_tuple()[:2] == (\"3\", \"11\")\n",
    "), \"You are not running Python 3.11. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cd02a028df69a1aa16eda29c6084a42",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nb_dirname = os.path.abspath(\"\")\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in [\n",
    "    \"IHA1\",\n",
    "    \"IHA2\",\n",
    "    \"HA1\",\n",
    "    \"HA2\",\n",
    "    \"HA3\",\n",
    "], \"[ERROR] The notebook appears to have been moved from its original directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1709bd6d2b55a83969e44d70763b1167",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "try:\n",
    "    display(\n",
    "        HTML(\n",
    "            r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(\n",
    "                nb_fname=nb_fname\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "except NameError:\n",
    "    assert False, \"Make sure to fill in the nb_fname variable above!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f405c9cd7b9720915f79dba54c89375",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "201f5ef89e0a5f1d489ddda4e6746469",
     "grade": false,
     "grade_id": "cell-ce4f9ca6e88f7e01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# IHA1 - Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b103e2eb1b7fc24904e3654028b0007",
     "grade": false,
     "grade_id": "cell-d1040a6bdfed8ae8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Welcome to the first individual home assignment!  \n",
    "\n",
    "This assignment consists of two parts:\n",
    " * Python and NumPy exercises;\n",
    " * Build a deep neural network for forward propagation.\n",
    "  \n",
    "The focus of this assignment is for you to gain practical knowledge with implementing forward and backward propagation of deep neural networks **without** using any deep learning framework. You will also gain practical knowledge in two of Python's scientific libraries, [NumPy](https://numpy.org/doc/1.26/) and [Matplotlib](https://matplotlib.org/devdocs/index.html). Note that NumPy and [PyTorch](https://pytorch.org/) (the deep learning framework used in this course) share a lot of similarities with how they handle arrays/tensors, so in the coming assignments, you can leverage on what you learn here.\n",
    "\n",
    "Skeleton code is provided for most tasks, and every part you are expected to implement is marked with **YOUR CODE HERE**. Throughout the assignment, you will also need to submit written answers to some questions. These questions are mainly to make you reflect on some particular topics, and your answers will not be graded in detail.\n",
    "\n",
    "We expect you to search and learn by yourself any commands you think are useful for these tasks. Don't limit yourself to what was taught in CL1. Use the help function, [stackoverflow](https://stackoverflow.com/), Google, the [Python documentation](https://docs.python.org/3.11/library/index.html) and the [NumPy](https://numpy.org/doc/2.0/index.html) documentation to your advantage.\n",
    "\n",
    "**IMPORTANT NOTE**: The tests available are not exhaustive, meaning that if you pass a test you have avoided the most common mistakes, but it is still not guaranteed that your solution is 100% correct. To pass this assignment, we expect you to pass all tests. \n",
    "\n",
    "While we use the tests for grading here, we encourage you to implement unit tests like these yourself when developing any code. Sanity-checking that your code behaves as expected can save you a lot of time down the road.\n",
    "\n",
    "Let's start by importing the necessary libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42431204d68f8d8b31e9306fd4cb6f0e",
     "grade": false,
     "grade_id": "cell-d2142353d71db431",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils.tests.iha1_tests as iha1_tests\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5a06a16327ea7c4a6cb5e9c32019609",
     "grade": false,
     "grade_id": "cell-03788efc69dbb922",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Lists and arrays introduction\n",
    "First, we will warm up with a Python exercise and few NumPy exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67d455883b08b0909e1f21c44bd09e9b",
     "grade": false,
     "grade_id": "cell-1eac31835165945d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 List comprehensions\n",
    "Examine the code snippet provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e520a0da079df88cebb5c5dcdeb4c3e",
     "grade": false,
     "grade_id": "cell-ee10027e05eff006",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def square_even_numbers(max_num=24):\n",
    "    my_list = []\n",
    "    for i in range(max_num + 1):\n",
    "        if i % 2 == 0:\n",
    "            my_list.append(i**2)\n",
    "    return my_list\n",
    "\n",
    "\n",
    "print(square_even_numbers(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "708ffda8b152deae5f5315135a5a1d49",
     "grade": false,
     "grade_id": "cell-04b896a3a8d65d13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This is not a very \"[pythonic](http://docs.python-guide.org/en/latest/writing/style/)\" way of writing. Lets re-write the code above using a [list comprehension](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions). The result will be less code, more readable and elegant. Your solution should be able to fit into one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d86577a30fb3ee8f7140b45c3b72d340",
     "grade": false,
     "grade_id": "cell-0fbb6ce83e4dcc85",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def square_even_numbers(max_num=24):\n",
    "    # YOUR CODE HERE\n",
    "    return my_list\n",
    "\n",
    "\n",
    "print(square_even_numbers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1037f32c2fc6823b7e70a0296ebe22c1",
     "grade": true,
     "grade_id": "cell-b836b8d25234359c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert square_even_numbers(1) == [0]\n",
    "assert square_even_numbers(24) == [\n",
    "    0,\n",
    "    4,\n",
    "    16,\n",
    "    36,\n",
    "    64,\n",
    "    100,\n",
    "    144,\n",
    "    196,\n",
    "    256,\n",
    "    324,\n",
    "    400,\n",
    "    484,\n",
    "    576,\n",
    "]\n",
    "assert len(square_even_numbers(-1)) == 0\n",
    "assert len(square_even_numbers(100)) == 51\n",
    "assert sorted(square_even_numbers(123)) == square_even_numbers(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98ef4e1d098e850cd0e494f8db283f25",
     "grade": false,
     "grade_id": "cell-b835734f0fb8c19c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Arrays, vectors, matrices, and tensors\n",
    "Many operations in deep learning revolve around handling and modifying data structures such as vectors (1D), matrices (2D), and tensors (N-D). In NumPy, all of these can be represented using arrays (in PyTorch, we'll use torch.Tensor's instead). Thus, it is important for you to understand these structures and the effects of applying different operations (matrix multiplications, reshaping, etc.).\n",
    "\n",
    "Run the cell below to create a numpy array.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d0f1424595c36bff4b85be37b6900e1",
     "grade": false,
     "grade_id": "cell-c391074193d1e66a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_arr = np.array([1, 9, 25, 49])\n",
    "print(my_arr)\n",
    "print(my_arr.shape)\n",
    "print(my_arr.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a29e2051700fb99218932792080c930",
     "grade": false,
     "grade_id": "cell-f62ee4516453d169",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The above result indicates that **my_arr** is an array of 4 elements with shape (4,) and one dimension. You might be temped to think of this as a vector, but the behavior of this array and a vector in the linear algebra sense can be very different. It is important to separate the two types because it will save a lot of debugging time later on. Read more about numpy shapes [here](https://stackoverflow.com/a/22074424).\n",
    "\n",
    "Run the code below to see how the transpose operation behaves differently between an array and vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e94268516c29fe5645a3d7aedaf6538",
     "grade": false,
     "grade_id": "cell-9ac01dc98f2aad70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# print the shape of an array and the shape of a transposed array\n",
    "print(\"my_arr is an array of shape:\")\n",
    "print(my_arr.shape)\n",
    "print(\"The transpose of my_arr has the shape:\")\n",
    "print(my_arr.T.shape)\n",
    "\n",
    "# print the shape of a column vector and the transpose of a column vector\n",
    "my_col_vec = my_arr.reshape(4, 1)\n",
    "print(\"my_col_vec is a vector of shape:\")\n",
    "print(my_col_vec.shape)\n",
    "print(\"The transpose of my_col_vec has the shape:\")\n",
    "print(my_col_vec.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2d675151f11a01d625261737d6e7def",
     "grade": false,
     "grade_id": "cell-1df1c59dcdb735cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As an effect, matrix multiplication between arrays of different shape can give very different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca673b6b1b145ff345b359ef85bd1bf2",
     "grade": false,
     "grade_id": "cell-e841a031baa39db3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"The matrix multiplication between my_arr and my_arr has shape:\")\n",
    "print((my_arr @ my_arr).shape)\n",
    "\n",
    "print(\"The matrix multiplication between my_arr and my_col_vec has shape:\")\n",
    "print((my_arr @ my_col_vec).shape)\n",
    "\n",
    "print(\"The matrix multiplication between my_col_vec.T and my_col_vec has shape:\")\n",
    "print((my_col_vec.T @ my_col_vec).shape)\n",
    "\n",
    "print(\"The matrix multiplication between my_col_vec and my_col_vec.T has shape:\")\n",
    "print((my_col_vec @ my_col_vec.T).shape)\n",
    "\n",
    "print(\"The matrix multiplication between my_col_vec and my_col_vec raises an error:\")\n",
    "try:\n",
    "    print(my_col_vec @ my_col_vec)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"The matrix multiplication between my_col_vec and my_arr raises an error:\")\n",
    "try:\n",
    "    print(my_col_vec @ my_arr)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8344d23da4be00b51790b759872956",
     "grade": false,
     "grade_id": "cell-8e264772240a7610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This, then has implications for multiplications with a matrix. One dimensional arrays can be both left-, and right multiplied with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4da22b8f55dd5fa2cf49f31aa0c66b3a",
     "grade": false,
     "grade_id": "cell-98a19e9d9519c8cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_matrix = np.random.rand(4, 4).round(2)  # We round for readability\n",
    "print(\"my_matrix is a matrix with the following values and shape:\")\n",
    "print(my_matrix)\n",
    "print(my_matrix.shape)\n",
    "\n",
    "arr_times_matrix = my_arr @ my_matrix\n",
    "matrix_times_arr = my_matrix @ my_arr\n",
    "print(\"The matrix multiplication between my_matrix and my_arr has shape:\")\n",
    "print((arr_times_matrix).shape)\n",
    "\n",
    "print(\"The matrix multiplication between my_arr and my_matrix has shape:\")\n",
    "print((matrix_times_arr).shape)\n",
    "\n",
    "print(\"But their results are not the same:\")\n",
    "print(f\"arr_times_matrix: {arr_times_matrix}\")\n",
    "print(f\"matrix_times_arr: {matrix_times_arr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f17d8e1d63fed6e70ef8d60a30b41e19",
     "grade": false,
     "grade_id": "cell-9ac8547eafce4bc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wheras the reshaped version behaves like a column vector as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3aa1e1a11a42b7f8cd7d092f6b67f1b",
     "grade": false,
     "grade_id": "cell-924b63b072344b0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "matrix_times_col_vec = my_matrix @ my_col_vec\n",
    "print(\n",
    "    f\"The matrix multiplication between my_matrix {my_matrix.shape} and my_col_vec {my_col_vec.shape} has shape:\"\n",
    ")\n",
    "print((matrix_times_col_vec).shape)\n",
    "print(\n",
    "    f\"Matrix times column vector gives the same values as the matrix times the array:\"\n",
    ")\n",
    "print((matrix_times_col_vec.flatten() == matrix_times_arr).all())\n",
    "\n",
    "row_vec_times_matrix = my_col_vec.T @ my_matrix\n",
    "print(\n",
    "    f\"The matrix multiplication between my_col_vec.T {my_col_vec.T.shape} and my_matrix {my_matrix.shape} has shape:\"\n",
    ")\n",
    "print((row_vec_times_matrix).shape)\n",
    "print(f\"Row vector times matrix gives the same values as the array times the matrix:\")\n",
    "print((row_vec_times_matrix.flatten() == arr_times_matrix).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d8d521be8486d4bef34e3dba646911c",
     "grade": false,
     "grade_id": "cell-2fd9f7dfb306a230",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To check your understanding, implement the following function which caluclates the resulting shape when multiplying two arrays of shape `(a, b)` and `(c, d)`. If the operation is not possible, return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11529c9add15a3e5442e22c81c9e8e19",
     "grade": false,
     "grade_id": "cell-3c0aa2db341ec50c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def out_shape(shape1, shape2):\n",
    "    \"\"\"\n",
    "    Calculate the shape of the resulting matrix when multiplying two arrays.\n",
    "\n",
    "    Arguments:\n",
    "    shape1 -- a tuple of 2 positive integers representing the shape of the first array\n",
    "    shape2 -- a tuple of 2 positive integers representing the shape of the second array\n",
    "\n",
    "    Returns:\n",
    "    A tuple of integers representing the shape of the resulting matrix\n",
    "    Returns None if the shapes are not compatible for matrix multiplication\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37ac4595f382f155080ff4e5730597d9",
     "grade": true,
     "grade_id": "cell-1f11b8bf0dcf8acd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(out_shape((1, 4), (4, 1)), tuple)\n",
    "assert out_shape((1, 4), (4, 1)) == (1, 1)\n",
    "assert out_shape((1, 1), (1, 1)) == (1, 1)\n",
    "assert out_shape((4, 1), (4, 1)) == None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f47499b6421113495c810f3a052ecf0a",
     "grade": false,
     "grade_id": "cell-7707c55f38c94d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a side-note, there are multiple different types of multiplication in NumPy such as [matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) (@), [multiply](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) (*), and [dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html). Make sure to use the right one for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fae6a10540571dbd3df91c9eec8d540d",
     "grade": false,
     "grade_id": "cell-954329c6d8a76b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Numpy exercises\n",
    "Now run the cell below to create the numpy array `numbers` and then complete the exercises sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a974d6601c83b8d357b9d7f734512773",
     "grade": false,
     "grade_id": "cell-b38cd50257c86b2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "numbers = np.arange(24)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6417057d4bd1511ff8ea538ce653f192",
     "grade": false,
     "grade_id": "cell-6a9944613f288397",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: reshape numbers into a 6x4 matrix\n",
    "# YOUR CODE HERE\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95c02d5ddcc34dcbc33ae60c84f48411",
     "grade": false,
     "grade_id": "cell-2881aa11ff8233be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample output from cell above for reference\n",
    "#  [[ 0  1  2  3]\n",
    "#   [ 4  5  6  7]\n",
    "#   [ 8  9 10 11]\n",
    "#   [12 13 14 15]\n",
    "#   [16 17 18 19]\n",
    "#   [20 21 22 23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cf94284e1441977000afb9b448b081e",
     "grade": true,
     "grade_id": "cell-0a93610d4c83f310",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_numpy_reshape(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a58f78882e9616bb458da70d83721616",
     "grade": false,
     "grade_id": "cell-eff4d59094285e57",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: set the element of the last row of the last column to zero\n",
    "# Hint: Try what happends when indices are negative\n",
    "# YOUR CODE HERE\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fc8f624ad62bdbd3230bcc36e95f200",
     "grade": false,
     "grade_id": "cell-5bd373983cf1ff38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample output from cell above for reference\n",
    "#  [[ 0  1  2  3]\n",
    "#   [ 4  5  6  7]\n",
    "#   [ 8  9 10 11]\n",
    "#   [12 13 14 15]\n",
    "#   [16 17 18 19]\n",
    "#   [20 21 22  0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2223d61d3a925022c54883b188e74eec",
     "grade": true,
     "grade_id": "cell-79d591ee24172650",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_numpy_neg_ix(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6ac367c23593e055aa7b9e333a671c6",
     "grade": false,
     "grade_id": "cell-3fa4e8328ca5052c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: set every element of the first row to 0\n",
    "# YOUR CODE HERE\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7aa19948d0e0db946d0004da9fa78925",
     "grade": false,
     "grade_id": "cell-9bfa316982252d14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample output from cell above for reference\n",
    "#  [[ 0  0  0  0]\n",
    "#   [ 4  5  6  7]\n",
    "#   [ 8  9 10 11]\n",
    "#   [12 13 14 15]\n",
    "#   [16 17 18 19]\n",
    "#   [20 21 22  0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5543f2d40a7b41b18ce7d6d2bc3555",
     "grade": true,
     "grade_id": "cell-1cd7674f74a06f63",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_numpy_row_ix(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1750a753162f953792c78b75609f6ec1",
     "grade": false,
     "grade_id": "cell-b033e238bf56428b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: append a 1x4 row vector of zeros to `numbers`,\n",
    "# resulting in a 7x4 matrix where the new row of zeros is the last row\n",
    "# Hint: A new matrix must be created in the procedure. Numpy arrays are not dynamic.\n",
    "# YOUR CODE HERE\n",
    "print(numbers)\n",
    "print(numbers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fce01cb07b9397eeab5e24e6357f4607",
     "grade": false,
     "grade_id": "cell-e7983d6d89cc8816",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample output from cell above for reference\n",
    "#   [[ 0  0  0  0]\n",
    "#    [ 4  5  6  7]\n",
    "#    [ 8  9 10 11]\n",
    "#    [12 13 14 15]\n",
    "#    [16 17 18 19]\n",
    "#    [20 21 22  0]\n",
    "#    [ 0  0  0  0]]\n",
    "#  (7, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a4e8659b9f35ce917b22e87d8a02827",
     "grade": true,
     "grade_id": "cell-4ce148f238ae1450",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_numpy_append_row(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "782e112a102c6f49b464d0dba6221f6b",
     "grade": false,
     "grade_id": "cell-e89d22af1e3a0609",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: set all elements with a value greater than 10 to the value 1\n",
    "# YOUR CODE HERE\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91d65cc5fafc129412efcf2c383f107c",
     "grade": false,
     "grade_id": "cell-f20e202dade5178c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample output from cell above for reference\n",
    "#  [[ 0  0  0  0]\n",
    "#   [ 4  5  6  7]\n",
    "#   [ 8  9 10  1]\n",
    "#   [ 1  1  1  1]\n",
    "#   [ 1  1  1  1]\n",
    "#   [ 1  1  1  0]\n",
    "#   [ 0  0  0  0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09b3b07af8792aa6ee22b5664afd1de5",
     "grade": true,
     "grade_id": "cell-bd6f406141034baa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_numpy_bool_matrix(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c482bf146fd85f25600e23dd25b44bc",
     "grade": false,
     "grade_id": "cell-4d0375ccc2a7ba35",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: compute the sum of every row and replace `numbers` with the answer\n",
    "# YOUR CODE HERE\n",
    "print(numbers.shape)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33edf7635477a2af2044f2a6d902fc96",
     "grade": false,
     "grade_id": "cell-87e014a2329b54bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample output from cell above for reference\n",
    "#   (7,)\n",
    "#   [ 0 22 28  4  4  3  0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f8da51452b0e3591d79b05a1be47337",
     "grade": true,
     "grade_id": "cell-81c52091be0324bf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_numpy_sum(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbbfa20d89a45cc5aa1afb28f558621f",
     "grade": false,
     "grade_id": "cell-fee8c66ec5e862c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Building your first neural network\n",
    "It is time to start implementing your first neural network. In this lab youw will focus on the fundamentals of forward and backpropagation. As the main goal is for you to proper learn the fundamentals and get a solid understanding, the neural networks in this lab are kept very simple. In upcoming labs you will build more advanced networks, but the principles learned in this lab still holds true for more complex problems and network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95503a7cdad7e8e6036eb176ee80c047",
     "grade": false,
     "grade_id": "cell-fce842bcd8962c7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Your first simple function (forward propagation)\n",
    "\n",
    "To start things off, we will begin with a simple scalar and linear function\n",
    "\\begin{equation}\n",
    "    y = w * x + b\n",
    "\\end{equation}\n",
    "where $x$ is some scalar input. As you might recall, this is very similar to the function represented by a single neuron in a network. Here we have left out the activation function for now, and only focus on the scalar case for simplicity.\n",
    "\n",
    "Start by implementing this function in the cell below. We call it forward as it's comparable to doing forward propagation for a single neuron (without any activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9302aa3ea35ac324550a1cb01f163182",
     "grade": false,
     "grade_id": "cell-de1862951d7cdb6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward(x, w, b):\n",
    "    \"\"\"Implement the forward pass of a linear neuron.\n",
    "\n",
    "    Arguments:\n",
    "    x - a scalar input of type 'float'\n",
    "    w - a scalar weight of type 'float'\n",
    "    b - a scalar bias of type 'float'\n",
    "\n",
    "    Returns:\n",
    "    y - the output of the neuron, a scalar of type 'float'\n",
    "    \"\"\"\n",
    "    y = None\n",
    "    # YOUR CODE HERE\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d40db3ba5dfcbe4b009762d4068e4729",
     "grade": false,
     "grade_id": "cell-964934c1da2ba3c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can visualize the function for some chosen input interval, and some choice of the weight $w$ and the bias $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e77c048a419cab99937be22d6e4ee6e",
     "grade": false,
     "grade_id": "cell-5bd8ac3c836a3364",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-3, 3, 1000)\n",
    "w = 5.0\n",
    "b = 2.0\n",
    "plt.plot(xs, forward(xs, w, b), label=f\"y = {w}x + {b}\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "262ecb99d73b80f9ca13866d065509cc",
     "grade": false,
     "grade_id": "cell-c9f8dc7c5406455f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's also add a data point $(x, y) = (1.5, 14.0)$ and visualize that in the same plot as our line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a51a7c76a1917e6920c52bda6c17d76c",
     "grade": false,
     "grade_id": "cell-11e92ab5cec05490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([1.5, 14.0])  # x, y\n",
    "plt.plot(xs, forward(xs, w, b), label=f\"y = {w}x + {b}\")\n",
    "plt.plot(data[0], data[1], \"ro\", label=f\"({data[0]}, {data[1]})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb1bfb0092973ca370dcadb0952fe840",
     "grade": false,
     "grade_id": "cell-383275ffc1c396bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see in the plot, our line doesn't quite go through the data point.\n",
    "\n",
    "Manually find some parameters for $w$ and $b$ such that your line fits the data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f31f0b92904cbc95c878bf5df7a31de5",
     "grade": false,
     "grade_id": "cell-6f00e3ecbb704543",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "plt.plot(xs, forward(xs, w, b), label=f\"y = {w}x + {b}\")\n",
    "plt.plot(data[0], data[1], \"ro\", label=f\"({data[0]}, {data[1]})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "314b64b488e4688b78f36ea13f485ee4",
     "grade": true,
     "grade_id": "cell-f0ef99ee9eac8a6c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert forward(data[0], w, b) == data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6deeca23de229494aefaaa9adda55db7",
     "grade": false,
     "grade_id": "cell-9b1e56faa7633e68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Using the derivative of the error\n",
    "\n",
    "We can also do something a bit more general and use the derivative of the error between our line and the point to find better parameters for our function.\n",
    "First, we will need to define some function that describes the error between our line and the point.\n",
    "Implement an error function that returns the squared error between some prediction from our function, $y_{pred}$, and some true value $y_{true}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69753dba567b3011c2d79eb3ef428e78",
     "grade": false,
     "grade_id": "cell-c6ef3eb09aeee42f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def error_function(y_pred, y_true):\n",
    "    \"\"\"Implement the error function as the squared error between the true and predicted value.\n",
    "\n",
    "    Arguments:\n",
    "    y_pred - a scalar value, the predicted value\n",
    "    y_true - a scalar value, the true value\n",
    "\n",
    "    Returns:\n",
    "    error - a scalar value, the squared error between the true and predicted value\n",
    "    \"\"\"\n",
    "    error = None\n",
    "    # YOUR CODE HERE\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09f46d080214fb3cce27e691e7f13688",
     "grade": true,
     "grade_id": "cell-64407e7b2807413e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (\n",
    "    np.abs(error_function(np.array(13.37), np.array(0.0)) - 178.75689999999997) < 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a35dc0673594b75d10c011a48014754a",
     "grade": false,
     "grade_id": "cell-0d99d17ab1d52fcb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we need to know how the error changes when our function changes. In other words, we need to know the derivative of the error function with respect to our function, $\\frac{\\partial e}{\\partial y}$.\n",
    "\n",
    "Implement $\\frac{\\partial e}{\\partial y}$ in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6528c971ac86a4c98d83f18bcb873198",
     "grade": false,
     "grade_id": "cell-e0661921a8581d64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def de_dy(y_pred, y_true):\n",
    "    \"\"\"Implement the derivative of the error function with respect to the predicted value.\n",
    "\n",
    "    Arguments:\n",
    "    y_pred - a scalar value, the predicted value\n",
    "    y_true - a scalar value, the true value\n",
    "\n",
    "    Returns:\n",
    "    derivative - a scalar value, the derivative of the error with respect to the predicted value\n",
    "    \"\"\"\n",
    "    derivative = None\n",
    "    # YOUR CODE HERE\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3f3859749885acccab58d1d42fb31ce",
     "grade": false,
     "grade_id": "cell-8cdd20d699f57939",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the derivative function for our original parameters w = 5.0, b = 2.0\n",
    "w, b = 5.0, 2.0\n",
    "x, y_true = data\n",
    "y_pred = forward(x, w, b)\n",
    "print(\"The error for the original parameters: \", error_function(y_pred, y_true))\n",
    "print(\"The derivative of the error with respect to y: \", de_dy(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b33224c4c2fa400659ed66251bde5ac",
     "grade": true,
     "grade_id": "cell-8b39d4d81eacb0a4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert error_function(y_pred, y_true) == 20.25\n",
    "assert de_dy(y_pred, y_true) == -9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c417478d4b0b3f4f79ebd9b5d918b34",
     "grade": false,
     "grade_id": "cell-88afc8d9afebd4a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What can you tell from this derivative? How can it guide you in moving your line? In particular, what does the sign of the derivative tell you? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd280e2ff459e3e95a05acb2af6c206",
     "grade": true,
     "grade_id": "cell-a157a52318c4994a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66b0b89803017b8657610e2d68a5d8dd",
     "grade": false,
     "grade_id": "cell-2fe760c1016e1ddd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What we are really interested in is how the error changes with respect to the parameters w and b. In the end, those are the parameters that we can change to reduce the error.\n",
    "\n",
    "We already know how the error changes with the output of our function from the exercise above. If we also manage to find out how our output changes with respect to our parameters $w$ and $b$, then we should in some magical way be able to finally compute how the error changes with respect to our parameters.\n",
    "\n",
    "In the cell below, implement the derivative of our function with respect to the parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b155ec2bde63f32827e2454f8cd5899",
     "grade": false,
     "grade_id": "cell-710430de9920825b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dy_dw(x):\n",
    "    \"\"\"Implement the derivative of the function with respect to the parameter w.\n",
    "\n",
    "    Arguments:\n",
    "    x - a scalar value, the input to the function\n",
    "\n",
    "    Returns:\n",
    "    derivative - a scalar value, the derivative of the function with respect to w\n",
    "    \"\"\"\n",
    "    derivative = None\n",
    "    # YOUR CODE HERE\n",
    "    return derivative\n",
    "\n",
    "\n",
    "def dy_db():\n",
    "    \"\"\"Implement the derivative of the function with respect to the parameter b.\n",
    "\n",
    "    Arguments:\n",
    "    x - a scalar value, the input to the function\n",
    "\n",
    "    Returns:\n",
    "    derivative - a scalar value, the derivative of the function with respect to b\n",
    "    \"\"\"\n",
    "    derivative = None\n",
    "    # YOUR CODE HERE\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "117dc7a2a5c55ad8f0992063f799e377",
     "grade": false,
     "grade_id": "cell-9fa9449ab3776f00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why doesn't the derivative of the function with respect to $b$ include $x$ as an input? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab84150920dce9e4dc818edfa1ef77dd",
     "grade": true,
     "grade_id": "cell-72c3c5aec0889372",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48aa518044dcf9c50ef104127ba65872",
     "grade": false,
     "grade_id": "cell-cea63b91fba209c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we know how the error changes with respect to the function, and how the function changes with respect to it's parameters, we should be able to somehow combine these results to compute how the error changes with respect to the parameters of our function.\n",
    "\n",
    "Implement the derivative of the error function with respect to the parameters $w$ and $b$ by using the results from the exercises above.\n",
    "\n",
    "HINT: The [chain rule](https://en.wikipedia.org/wiki/Chain_rule) from your old calculus class is your friend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90657589364bf09cb87f68ebda5f3684",
     "grade": false,
     "grade_id": "cell-65a5c3eff842eed5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def de_dw(x, y_pred, y_true):\n",
    "    \"\"\"Implement the derivative of the error function with respect to the parameter w.\n",
    "\n",
    "    Arguments:\n",
    "    x - a scalar value, the input to the function\n",
    "    y_pred - a scalar value, the predicted value\n",
    "    y_true - a scalar value, the true value\n",
    "\n",
    "    Returns:\n",
    "    derivative - a scalar value, the derivative of the function with respect to b\n",
    "    \"\"\"\n",
    "    derivative = None\n",
    "    # YOUR CODE HERE\n",
    "    return derivative\n",
    "\n",
    "\n",
    "def de_db(y_pred, y_true):\n",
    "    \"\"\"Implement the derivative of the error function with respect to the parameter b.\n",
    "\n",
    "    Arguments:\n",
    "    y_pred - a scalar value, the predicted value\n",
    "    y_true - a scalar value, the true value\n",
    "\n",
    "    Returns:\n",
    "    derivative - a scalar value, the derivative of the function with respect to b\n",
    "    \"\"\"\n",
    "    derivative = None\n",
    "    # YOUR CODE HERE\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "114f630e56e55e9d4358e319b645b1a0",
     "grade": false,
     "grade_id": "cell-655742e8536572d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"The error for the original parameters: \", error_function(y_pred, y_true))\n",
    "print(\"The derivative of the error with respect to w: \", de_dw(x, y_pred, y_true))\n",
    "print(\"The derivative of the error with respect to b: \", de_db(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "682c75f2ce42c401d9b755dd1440e004",
     "grade": true,
     "grade_id": "cell-8734f0f9a0cd8f84",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert de_dw(x, y_pred, y_true) == -13.5\n",
    "assert de_db(y_pred, y_true) == -9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d571e9e70f1db4f8438e55daa851e0ed",
     "grade": false,
     "grade_id": "cell-8299d4903eef1135",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What does these derivatives tell you? How would you use them to update the parameters $w$ and $b$ such that the error is reduced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1722ab81eff3cfbd1003bfc630937a",
     "grade": true,
     "grade_id": "cell-d55bf311f0ba6471",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23541f858b32631f494226b622b6bad8",
     "grade": false,
     "grade_id": "cell-d721e4a3c6adae5e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will now implement a function for updating the parameters based on the derivatives you calculated in the previous cell. Move the parameters in the direction, such that the error is decreased. The function includes a step size as an input, which scales the size of the update step. You can keep this as the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73fa3a3b3606e75c0a839c05d94630fa",
     "grade": false,
     "grade_id": "cell-d0d0f216fde84bb1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_params(w, b, de_dw, de_db, step_size=0.1):\n",
    "    \"\"\"Implement the update step of parameters w and b, based on the derivatives of the error function with respect to these parameters.\n",
    "\n",
    "    Arguments:\n",
    "    w - a scalar value, the weight parameter\n",
    "    b - a scalar value, the bias parameter\n",
    "    de_dw - a scalar value, the derivative of the error with respect to w\n",
    "    de_db - a scalar value, the derivative of the error with respect to b\n",
    "    step_size - a scalar value, the step size for the update step\n",
    "\n",
    "    Returns:\n",
    "    w - a scalar value, the updated weight parameter\n",
    "    b - a scalar value, the updated bias parameter\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dabbc5106027646c48add68829a6f06f",
     "grade": false,
     "grade_id": "cell-777d524e7827821a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If implemented correctly, we should see a decrease in the error after one update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eec9a438357d7a55fdc786a9f31721b8",
     "grade": false,
     "grade_id": "cell-4209779e15ce593b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"The original parameters: \", w, b)\n",
    "print(\"The error for the original parameters: \", error_function(y_pred, y_true))\n",
    "\n",
    "w_updated, b_updated = update_params(\n",
    "    w, b, de_dw(x, y_pred, y_true), de_db(y_pred, y_true)\n",
    ")\n",
    "print(\"The updated parameters: \", w_updated, b_updated)\n",
    "y_pred_updated = forward(x, w_updated, b_updated)\n",
    "print(\"The error for the updated parameters: \", error_function(y_pred_updated, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "875cec97faecefff9ba42ae5fca925d4",
     "grade": true,
     "grade_id": "cell-fbdc079d92b6c9f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert error_function(y_pred_updated, y_true) < error_function(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "defee98d642893c45158ddb37e14d451",
     "grade": false,
     "grade_id": "cell-204893183e61fd7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why do we use a step size when updating the parameters instead of using the gradients directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "707660bcaf7db7b95272101200bb0d4d",
     "grade": true,
     "grade_id": "cell-bdc47086c5102c83",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfeb50862a1648ed3e64d8ba6e3f449f",
     "grade": false,
     "grade_id": "cell-2d7186240d1fcddc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you might have noticed, a single update step was not enough to find a good fit. Instead, let us iteratively update our parameters, where each iteration uses the updated parameters from the last iteration to compute a new error and derivatives.\n",
    "\n",
    "Implement a function that iteratively updates your parameters, using the functions you have implemented earlier in the lab. Use the default values for `iterations` and `step_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d075e8daf8a6009b9e38401ec225f158",
     "grade": false,
     "grade_id": "cell-e7a6cb8844b74353",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_parameters(x, y_true, w, b, iterations=10, step_size=0.1):\n",
    "    \"\"\"Implement a function that iteratively updates the parameters w and b.\n",
    "\n",
    "    Arguments:\n",
    "    x - a scalar value, the input to the function\n",
    "    y_true - a scalar value, the true value\n",
    "    w - a scalar value, the weight parameter\n",
    "    b - a scalar value, the bias parameter\n",
    "    iterations - a scalar value, the number of iterations for the optimization\n",
    "    step_size - a scalar value, the step size for the update step\n",
    "\n",
    "    Returns:\n",
    "    w - a scalar value, the updated weight parameter\n",
    "    b - a scalar value, the updated bias parameter\n",
    "    \"\"\"\n",
    "    for i in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        print(f\"Iteration {i} | Error: \", error_function(y_pred, y_true))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7e34ec17653d5b3e952852904da46d3",
     "grade": false,
     "grade_id": "cell-0afec84a3112cd75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your iterative optimizer on the initial parameters from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e016dab055577ea102d63497a4261b4",
     "grade": false,
     "grade_id": "cell-5cc4d5fdaf667469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "w_optimized, b_optimized = optimize_parameters(data[0], data[1], 5.0, 2.0)\n",
    "plt.plot(\n",
    "    xs,\n",
    "    forward(xs, w_optimized, b_optimized),\n",
    "    label=f\"y = {round(w_optimized, 3)}x + {round(b_optimized, 3)}\",\n",
    ")\n",
    "plt.plot(data[0], data[1], \"ro\", label=f\"({data[0]}, {data[1]})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd4a450e4cb74c60327b3425c31908e7",
     "grade": false,
     "grade_id": "cell-cf0d5fddf5af4364",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hopefully, you should see a rapid decrease of the error in the first few iterations, and the resulting error should be close to zero after 10 iterations.\n",
    "\n",
    "How would a different step size affect the optimization? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c489059913e3c01a472fda8535a9f2ee",
     "grade": true,
     "grade_id": "cell-b2cd4cf6414abcca",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3220bf5248a1e203fb6fa83fd789c80c",
     "grade": false,
     "grade_id": "cell-4aa17410668bf08e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The process we have just implemented is called gradient descent. It is a simple optimization algorithm that is used to minimize a function by iteratively moving in the direction of steepest descent.\n",
    "In our case, the function we are trying to minimize is the error between our line and the point, and the parameters we are optimizing are w and b.\n",
    "The way we propagate the derivative of the error throughout the function to update the parameters is called backpropagation. It is a key concept in neural networks and deep learning, but is as simple as the chain rule in calculus at its core.\n",
    "In deep learning, we usually refer to the error function as the loss function, and the derivatives that we have propagated from our loss function to our parameters are referred to as gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7077545b06fb846f40aca059d0ed6c78",
     "grade": false,
     "grade_id": "cell-a2ab337a3528de1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 Activation functions\n",
    "The function we have used so far is not very exciting, and only lets us describe the rather boring family of linear functions. As you have already seen in the video and lecture material, we add an activation function to our otherwise linear neuron to be able to also model non-linear functions.\n",
    "\n",
    "In the coming part of the lab we will use the notation that you might find familiar from the lectures, where $z = w*x + b$ denotes the output before activation, and $a = g(z)$ denotes the output after activation.\n",
    "\n",
    "Lets start with implementing some common activation functions. To make them slightly more useful, we ask you to implement them such that they can handle vectorized input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b95b221cc25c88dcb10335df506caf78",
     "grade": false,
     "grade_id": "cell-f9e518c127e6c546",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Due to the change of notation, we can make the following alias to simplify things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0be3808ad3e7ae9f0b6e14590e5540f7",
     "grade": false,
     "grade_id": "cell-237a5dea1981a14e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "de_dz = de_dy\n",
    "dz_dw = dy_dw\n",
    "dz_db = dy_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09d1bd17b8ec17c30297184e78b16974",
     "grade": false,
     "grade_id": "cell-92bec37feaa6435c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.3.1 ReLU\n",
    "The neural network will use the ReLU activation function in every layer except for the last. ReLU does element-wise comparison of the input matrix. For example, if the input is `X`, and `X[i,j] == 2` and `X[k,l] == -1`, then after applying ReLU, `X[i,j] == 2` and `X[k,l] == 0` should be true.  \n",
    "\n",
    "The formula for implementing ReLU for a single neuron $i$ is:\n",
    "\\begin{equation}\n",
    "relu(z_i) = \n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ z_i \\leq 0 \\\\\n",
    "      z_i, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Now implement `relu` in vectorized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0765b09f7986fe0d3c76e28e0a879bca",
     "grade": false,
     "grade_id": "cell-6b722cbfac9a7e7f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"Implement the ReLU activation function\n",
    "\n",
    "    Arguments:\n",
    "    z - the input of the activation function. Has a type of 'numpy.ndarray'\n",
    "\n",
    "    Returns:\n",
    "    a - the output of the activation function. Has a type of numpy.ndarray and the same shape as 'z'\n",
    "    \"\"\"\n",
    "\n",
    "    a = None  # TODO\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86c28436ecd0b71c43f94a37cd419f6a",
     "grade": true,
     "grade_id": "cell-1b60c756c7ca0682",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_relu_forward(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0887b1c7ca15b311ff895ede5ed9e8a7",
     "grade": false,
     "grade_id": "cell-c8eb22c2ceaf2462",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Can you name some of the advantages of the ReLU activation function? (We will not grade this answer in detail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "820a9978f1609e7edf6e3212d32635f0",
     "grade": true,
     "grade_id": "cell-af321f690002c47c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29934bd818d290061a96a11e87293329",
     "grade": false,
     "grade_id": "cell-584c824ee1a8d486",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.3.2 Sigmoid\n",
    "The sigmoid activation function is common for binary classification. This is because it squashes its input to the range [0,1].  \n",
    "Implement the activation function `sigmoid` using the formula:  \n",
    "\\begin{equation}\n",
    "    \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326c6ed93f7e37444c6101c22db605ad",
     "grade": false,
     "grade_id": "cell-f078a8eaa7d1613e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Implement the sigmoid activation function\n",
    "\n",
    "    Arguments:\n",
    "    z - the input of the activation function. Has a type of 'numpy.ndarray'\n",
    "\n",
    "    Returns:\n",
    "    a - the output of the activation function. Has a type of 'numpy.ndarray' and the same shape as 'z'\n",
    "    \"\"\"\n",
    "\n",
    "    a = None  # TODO\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "227e7a74d2d820a8d04aaf556abf7dfe",
     "grade": true,
     "grade_id": "cell-708cc751b746badc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f91c9eacd6cac190d032d5aef4ec87e",
     "grade": false,
     "grade_id": "cell-bd86ff6065a6f326",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why is the sigmoid activation function useful for binary classification? Feel free to use some numerical examples to show how the magnitudes of the variable $z$ affect the output of the sigmoid layer. (We will not grade this answer in detail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c3a7f7091c06dd5ffa5f1bf8e0f3ca0",
     "grade": true,
     "grade_id": "cell-f3cfee39e4ca3615",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4d7b7343780ad6de366eb71f2585316",
     "grade": false,
     "grade_id": "cell-22a36d73cc8f651f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.3.3 Visualization\n",
    "Make a plot using matplotlib to visualize the activation functions between the input interval [-3,3]. The plot should have the following properties\n",
    " * one plot should contain a visualization of both `ReLU` and `sigmoid`;\n",
    " * x-axis: range of values between [-3,3], **hint**: np.linspace;\n",
    " * y-axis: the value of the activation functions at a given input `x`;\n",
    " * a legend explaining which line represents which activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1651a6aef6835d89cb841d3a3f013793",
     "grade": false,
     "grade_id": "cell-19b7c28a99b434f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: make a plot of ReLU and sigmoid values in the interval [-3,3]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5093fc378885efcb161730f47d5a8204",
     "grade": false,
     "grade_id": "cell-75f2133086de2a05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.3.4 Softmax\n",
    "Another common activation function is the softmax activation function. It is often used for the final layer of the neural network in classification tasks. Implement `softmax` according to the formula below. The subtraction of the maximum value is there solely to avoid overflows in a practical implementation.\n",
    "\\begin{equation}\n",
    "softmax(z_i) = \\frac{e^{z_i - max(\\mathbf{z})}}{ \\sum^j e^{z_j - max(\\mathbf{z})}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff7fea29808121a4672459489a313f36",
     "grade": false,
     "grade_id": "cell-20b0325adf95cafa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Implement the softmax activation function\n",
    "\n",
    "    Arguments:\n",
    "    z - the input of the activation function, shape (BATCH_SIZE, FEATURES) and type 'numpy.ndarray'\n",
    "\n",
    "    Returns:\n",
    "    a - the output of the activation function, shape (BATCH_SIZE, FEATURES) and type 'numpy.ndarray'\n",
    "    \"\"\"\n",
    "\n",
    "    a = None  # TODO\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d6e297d80ada5543c0a1c4ebcb4f42a",
     "grade": true,
     "grade_id": "cell-d8a3083df1819c28",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_softmax(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "304dd09a00a4c3ac486a0849d058e553",
     "grade": false,
     "grade_id": "cell-53a9859a0003ac9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What are the main differences between using `sigmoid` and `softmax` for multi-class classification problems? And when the number of classes is 2? (We will not grade this answer in detail.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8552b982b979ae39b6db05941c32cdbc",
     "grade": true,
     "grade_id": "cell-d78bfdbb8c0adbe7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7368be122ec9ea1eace543dd3a32941",
     "grade": false,
     "grade_id": "cell-96a46b8772eeaa95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.4 Putting the pieces together\n",
    "\n",
    "Now you have implemented, and hopefully understood, the building blocks needed to build your first neural network. All that is left now is putting these pieces together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a21b4c524a61bd4b5c773d7f50e4b6d",
     "grade": false,
     "grade_id": "cell-d12011158b313809",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.4.1 Implementing a layer\n",
    "\n",
    "You will start with implementing a `Layer`-class. This will be convenient when later constructing your neural network. To keep things simple, and make use of the functions you've implemented earlier in this lab, we will stick to scalar input and having one neuron per layer. However, you will also add `ReLU` as an activation function this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87210db32b3ffd7fcfc3b9bd01de0357",
     "grade": false,
     "grade_id": "cell-81d592325f73d8bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Since you now need to also propagate your derivatives through the activation function, start with implementing the derivative of `ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fec434d2b306773eeb9439f8ee73a260",
     "grade": false,
     "grade_id": "cell-29051410ea238e1b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    \"\"\"Implement the derivative of the ReLU activation function\n",
    "\n",
    "    Arguments:\n",
    "    x - the input of the activation function, shape (BATCH_SIZE, FEATURES) and type 'numpy.ndarray'\n",
    "\n",
    "    Returns:\n",
    "    dx - the derivative of the activation function, shape (BATCH_SIZE, FEATURES) and type 'numpy.ndarray'\n",
    "    \"\"\"\n",
    "    dx = None\n",
    "    # YOUR CODE HERE\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39a97e5e6ee9927a8202dcea10018309",
     "grade": true,
     "grade_id": "cell-bf1dc9ecd4ec74c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert relu_derivative(np.array(-1)) == 0\n",
    "assert relu_derivative(np.array(1)) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20dad68af369b54bc2641c721a6d3af9",
     "grade": false,
     "grade_id": "cell-0d38094e15b7c999",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now it's time to fill in the missing pieces of the `Layer`-class. The parameters of the layer should be initialized with random values. Think about how to initialize your parameters in a way that make sense with the chosen activation function (ReLU in this case). Hint: Look at the local gradient of ReLU and think about what ranges of values this gradient will be active and how it will affect the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ac14989cfd8e8cae7488450a2b8c419",
     "grade": false,
     "grade_id": "cell-b752ec1919948cb2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Implement a class for a neural network layer.\n",
    "\n",
    "    The class should implement the forward and backward passes of the layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Implement the initialization of the layer parameters.\n",
    "\n",
    "        Arguments:\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Parameters of the layer. Should be initialized randomly.\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Activation function for the layer.\n",
    "        self.activation = relu\n",
    "        self.activation_derivative = relu_derivative\n",
    "\n",
    "        # Attributes to store the input and output of a layer during forward pass, for use in the backward passes.\n",
    "        self.last_x = None\n",
    "        self.last_a = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Implement the forward pass of the layer.\n",
    "\n",
    "        Arguments:\n",
    "        x - the input to the layer, scalar of type 'float'\n",
    "\n",
    "        Returns:\n",
    "        a - the output of the layer, scalar of type 'float'\n",
    "        \"\"\"\n",
    "        a = None\n",
    "        # YOUR CODE HERE\n",
    "        self.last_x = x\n",
    "        self.last_a = a\n",
    "        return a\n",
    "\n",
    "    def backward(self, de_da, step_size=0.01):\n",
    "        \"\"\"Implement the backward pass of the layer. The backward pass should update the parameters of\n",
    "        the layer and return the derivative of the error with respect to the input of the layer.\n",
    "\n",
    "        Arguments:\n",
    "        de_da - the derivative of the error with respect to the output of the layer, scalar of type 'float'\n",
    "        step_size - the step size for the update step, scalar of type 'float'\n",
    "\n",
    "        Returns:\n",
    "        de_dx - the derivative of the error with respect to the input of the layer, scalar of type 'float'\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return de_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48a5f65a0d37b0ae156a640a7db9a8dd",
     "grade": true,
     "grade_id": "cell-2cddf0bb0770c745",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_layer = Layer()\n",
    "\n",
    "# make sure w and b are initialized\n",
    "assert test_layer.w is not None\n",
    "assert test_layer.b is not None\n",
    "\n",
    "# set to some values for tests\n",
    "test_layer.w = np.array(10.0)\n",
    "test_layer.b = np.array(-1.0)\n",
    "\n",
    "# assert correct forward for some input\n",
    "assert test_layer.forward(np.array(42.0)) == 419.0\n",
    "assert test_layer.last_x == 42.0\n",
    "assert test_layer.last_a == 419.0\n",
    "\n",
    "# assert correct backward\n",
    "assert test_layer.backward(np.array(42.0), step_size=0.01) == 420\n",
    "assert np.abs(test_layer.w - (-7.64)) < 0.01\n",
    "assert np.abs(test_layer.b - (-1.42)) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fe70ff8e007ce55c3f8136ab3bc58f4",
     "grade": false,
     "grade_id": "cell-7642ab46539e43b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.4.2 Building your network\n",
    "Almost there! Now we just need to chain a bunch of layers together and we have ourselves a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f8041a40ae07e4e41297678be3940fc",
     "grade": false,
     "grade_id": "cell-6b226a094fd3c088",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"Implement a class for a neural network.\n",
    "\n",
    "    The class should implement the forward and backward passes of the neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Implement the forward pass of the neural network. The neural network should pass the input through all layers.\n",
    "\n",
    "        Arguments:\n",
    "        x - the input to the neural network, scalar of type 'float'\n",
    "\n",
    "        Returns:\n",
    "        x - the output of the neural network, scalar of type 'float'\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return x\n",
    "\n",
    "    def backward(self, error_derivative, step_size=0.01):\n",
    "        \"\"\"Implement the backward pass of the neural network. The neural network should pass the error derivative through\n",
    "        all layers and update the parameters of each layer.\n",
    "\n",
    "        Arguments:\n",
    "        error_derivative - the derivative of the error with respect to the output of the neural network, scalar of type 'float'\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "865a8229ecb91de5d962ae6f605e9f64",
     "grade": false,
     "grade_id": "cell-a93e375d48b15a0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lets test the forward pass of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80629c61874f71a7cb5d2b1c1c397755",
     "grade": false,
     "grade_id": "cell-9a70431deef9f5f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork()\n",
    "layer1 = Layer()\n",
    "layer2 = Layer()\n",
    "neural_network.add_layer(layer1)\n",
    "neural_network.add_layer(layer2)\n",
    "\n",
    "y_pred = neural_network.forward(x)\n",
    "print(\"The error for the original parameters: \", error_function(y_pred, y_true))\n",
    "plt.plot(xs, neural_network.forward(xs), label=f\"neural network\")\n",
    "plt.plot(data[0], data[1], \"ro\", label=f\"({data[0]}, {data[1]})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d6a880357b8f7fa4e803a80fdb4d1e0",
     "grade": false,
     "grade_id": "cell-c80806e4d40531ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.4.3 Training your network\n",
    "Now let's also implement the iterative optimization that we did before. Here `y_pred` refers to the output from the last layer of your network, commonly referred to as the prediction.\n",
    "\n",
    "In deep learning, we usually refer to the optimization process as \"training\" the network. Thus, the function to iteratively update the parameters of your network is here called `train`, but the principle remains the same as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bef6a850e308d53fbc67b868013d7dfb",
     "grade": false,
     "grade_id": "cell-fe4f3896a5a08b8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(x, y_true, neural_network, iterations=10, step_size=0.01):\n",
    "    \"\"\"Implement the training loop for the neural network. The training loop should update the parameters of the neural network\n",
    "    based on the error between the predicted and true values.\n",
    "\n",
    "    Arguments:\n",
    "    x - the input to the neural network, scalar of type 'float'\n",
    "    y_true - the true value of the output, scalar of type 'float'\n",
    "    neural_network - the neural network object\n",
    "    iterations - the number of training iterations, scalar of type 'int'\n",
    "    step_size - the step size for the update step, scalar of type 'float'\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        print(f\"Iteration {i} | Error: \", error_function(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec90bc2f94b0e0779f53bc8fa14bbbf5",
     "grade": false,
     "grade_id": "cell-80625574fc135517",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we can test the training loop on the same simple example as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35d3d7eeff3a1af4f7c959fb8084e385",
     "grade": false,
     "grade_id": "cell-cffb872548d6a4cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train(data[0], data[1], neural_network)\n",
    "\n",
    "plt.plot(xs, neural_network.forward(xs), label=f\"neural network\")\n",
    "plt.plot(data[0], data[1], \"ro\", label=f\"({data[0]}, {data[1]})\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39a834dbb8d750ba6fe57f9c6658c65d",
     "grade": false,
     "grade_id": "cell-612d96eb90d2c409",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Hopefully, you should again see the error decrease at each iteration of our training loop. In the end, the error should be close to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0836dc7521c21d44165616155d150df",
     "grade": false,
     "grade_id": "cell-ce7909395805d2f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Building a deep neural network\n",
    "\n",
    "Very few problems are one-dimensional and cannot be solved with the simple scalar network we have implemented so far. \n",
    "Thus, as the final part of this assignment, you will now implement the `Layer` and `NeuralNetwork` classes such that they support multi-dimensional input, and any choice of activation function.\n",
    "However, you will be given code for the backward propagation and only need to implement the forward propagation.\n",
    "\n",
    "(If you want to try to derive the backward propagation yourself [this document](https://compsci697l.github.io/docs/vecDerivs.pdf) could be helpful to simplify the derivatives for higher dimensions)\n",
    "\n",
    "Recall the formula for forward propagation of an arbitrary layer $l$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{a}^{[l]} = g(\\mathbf{z}^{[l]}) = g(\\mathbf{a}^{[l-1]}\\mathbf{w}^{[l]} +\\mathbf{b}^{[l]})\n",
    "\\end{equation}\n",
    "\n",
    "where $g$ is the activation function given by `activation_fn`, which can be relu, sigmoid or softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e7035206dd866a2ebdbe971861a99b8",
     "grade": false,
     "grade_id": "cell-f9f037797365efab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Initialize weights\n",
    "You will first implement a helper function that takes the shape of a layer as input, and returns an initialized weight matrix $\\mathbf{W}$ and bias vector $\\mathbf{b}$ as output. The matrix $\\mathbf{W}$ should be sampled from a normal distribution with mean 0 and standard deviation 2, and $\\mathbf{b}$ should be initialized to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca5843d587f068f432f904eb091804e1",
     "grade": false,
     "grade_id": "cell-148ccf48c2171bf5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(layer_shape):\n",
    "    \"\"\"Implement initialization of the weight matrix and biases\n",
    "\n",
    "    Arguments:\n",
    "    layer_shape - a tuple of length 2, type (int, int), that determines the dimensions of the weight matrix: (input_dim, output_dim)\n",
    "\n",
    "    Returns:\n",
    "    w - a weight matrix with dimensions of 'layer_shape', (input_dim, output_dim), that is normally distributed with\n",
    "        properties mu = 0, stddev = 2. Has a type of 'numpy.ndarray'\n",
    "    b - a vector of initialized biases with shape (1,output_dim), all of value zero. Has a type of 'numpy.ndarray'\n",
    "    \"\"\"\n",
    "    w = None  # TODO\n",
    "    b = None  # TODO\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "782cb90e4e9d5b270e441c468f5e2e16",
     "grade": true,
     "grade_id": "cell-5c2d876b70fbe262",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_initialize_weights(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92769a975d474e1374cecdcbd0371d4a",
     "grade": false,
     "grade_id": "cell-6bd130c4bd377cd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Implementing the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72d1ae0b842c10da198444e45cba33d7",
     "grade": false,
     "grade_id": "cell-eb356a6e3839061b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    TODO: Build a class called Layer that satisfies the descriptions of the methods\n",
    "    Make sure to utilize the helper functions you implemented before\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation_fn=relu):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        input_dim - the number of inputs of the layer. type int\n",
    "        output_dim - the number of outputs of the layer. type int\n",
    "        activation_fn - a reference to the activation function to use. Should be 'relu' as a default\n",
    "                        possible values are the 'relu', 'sigmoid' and 'softmax' functions you implemented earlier.\n",
    "                        Has the type 'function'\n",
    "\n",
    "        Attributes:\n",
    "        w - the weight matrix of the layer, should be initialized with 'initialize_weights'\n",
    "            and has the shape (INPUT_FEATURES, OUTPUT_FEATURES) and type 'numpy.ndarray'\n",
    "        b - the bias vector of the layer, should be initialized with 'initialize_weights'\n",
    "            and has the shape (1, OUTPUT_FEATURES) and type 'numpy.ndarray'\n",
    "        activation_fn - a reference to the activation function to use.\n",
    "                        Has the type 'function'\n",
    "        \"\"\"\n",
    "        self.w, self.b = None, None  # TODO\n",
    "        self.activation_fn = None  # TODO\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward_prop(self, a_prev):\n",
    "        \"\"\"Implement the forward propagation module of the neural network layer\n",
    "        Should use whatever activation function that 'activation_fn' references to\n",
    "\n",
    "        Arguments:\n",
    "        a_prev - the input to the layer, which may be the data 'X', or the output from the previous layer.\n",
    "            a_prev has the shape of (BATCH_SIZE, INPUT_FEATURES) and the type 'numpy.ndarray'\n",
    "\n",
    "        Returns:\n",
    "        a - the output of the layer when performing forward propagation. Has the type 'numpy.ndarray'\n",
    "        \"\"\"\n",
    "\n",
    "        a = None  # TODO\n",
    "        # YOUR CODE HERE\n",
    "        self.input = a_prev\n",
    "        self.output = a\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward_prop(self, output_gradient, learning_rate):\n",
    "        if self.activation_fn == relu:\n",
    "            output_gradient *= relu_derivative(self.output)\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "        input_gradient = np.dot(output_gradient, self.w.T)\n",
    "        self.w -= learning_rate * weights_gradient\n",
    "        self.b -= learning_rate * np.sum(output_gradient, axis=0, keepdims=True)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5c06a1dc78614dfe52d376ff6e446b5",
     "grade": true,
     "grade_id": "cell-df50da02535032fa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case, be sure that you pass the previous activation function tests before running this test\n",
    "iha1_tests.test_layer(Layer, relu, sigmoid, softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88c96261c856be8666a17778836d1bab",
     "grade": false,
     "grade_id": "cell-a9e83360102356ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3 Implementing the neural network\n",
    "You will now define the actual neural network class. It is an L-layer neural network, meaning that the number of layers and neurons in each layer is specified as input by the user. Once again, you will only focus on implementing the forward propagation part.\n",
    "\n",
    "Read the descriptions in the comments and complete the **TODO**s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f2471faeab2bb81146b4b1bcb75b94f",
     "grade": false,
     "grade_id": "cell-d4523402d5cc2b0f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    TODO: Implement an L-layer neural network class by utilizing the Layer module defined above\n",
    "    Each layer should use 'relu' activation function, except for the output layer, which should use 'softmax'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_n, layer_dims):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        input_n    - the number of inputs to the network. Should be the same as the length of a data sample\n",
    "                     Has type int\n",
    "        layer_dims - a python list or tuple of the number of neurons in each layer. Layer 'l' should have a weight matrix\n",
    "                     with the shape ('layer_dims[l-1]', 'layer_dims[l]').\n",
    "                     'layer_dims[-1]' is the dimension of the output layer.\n",
    "                     Layer 1 should have the dimensions ('input_n', 'layer_dims[0]').\n",
    "                     len(layer_dims) is the depth of the neural network\n",
    "        Attributes:\n",
    "        input_n - the number of inputs to the network. Has type int\n",
    "        layers  - a python list of each layer in the network. Each layer should use the 'relu' activation function,\n",
    "                  except for the last layer, which should use 'softmax'.\n",
    "                  Has type 'list' containing layers of type 'Layer'\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_n = None  # TODO\n",
    "        self.layers = None  # TODO\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        Implement the forward propagation procedure through the entire network, from input to output.\n",
    "        You will now connect each layer's forward propagation function into a chain of layer-wise forward propagations.\n",
    "\n",
    "        Arguments:\n",
    "        x - the input data, which has the shape (BATCH_SIZE, NUM_FEATURES) and type 'numpy.ndarray'\n",
    "\n",
    "        Returns:\n",
    "        a - the output of the last layer after forward propagating through the every layer in 'layers'.\n",
    "            Should have the dimension (BATCH_SIZE, layers[-1].w.shape[1]) and type 'numpy.ndarray'\n",
    "        \"\"\"\n",
    "        a = None  # TODO\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward_prop(self, output_gradient, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            output_gradient = layer.backward_prop(output_gradient, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d827b5240cc7c8e29b01387dbe3bed48",
     "grade": true,
     "grade_id": "cell-d595c3f39a45382b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test case\n",
    "iha1_tests.test_neuralnetwork(NeuralNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a9ee085801da6f53a634b848bbc8f36",
     "grade": false,
     "grade_id": "cell-955ba52bafc16aa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.4 Training\n",
    "\n",
    "All that is left now is the training loop for your network. You are already given a loss function and its derivative (loss is just another word for error) that will be suitable for the upcoming problem that you will train your network on. Implement the missing steps in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "831731dfb5eda89bbaac467e5e5714d0",
     "grade": false,
     "grade_id": "cell-9df0262484cb1617",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(output, y_true, epsilon=1e-10):\n",
    "    p = np.clip(output, epsilon, 1 - epsilon)\n",
    "    log_likelihood = -np.sum(y_true * np.log(p)) / y_true.shape[0]\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def compute_loss_gradient(output, y_true):\n",
    "    return (output - y_true) / y_true.shape[0]\n",
    "\n",
    "\n",
    "def train(neural_network, x_train, y_train, iterations, learning_rate):\n",
    "    \"\"\"Implement the training loop for the neural network. The training loop should update the parameters of the neural network\n",
    "    based on the error between the predicted and true values.\n",
    "\n",
    "    Arguments:\n",
    "    neural_network - the neural network object\n",
    "    x_train - the input to the neural network, shape (BATCH_SIZE, INPUT_FEATURES) and type 'numpy.ndarray'\n",
    "    y_train - the true value of the output, shape (BATCH_SIZE, OUTPUT_FEATURES) and type 'numpy.ndarray'\n",
    "    iterations - the number of training iterations, scalar of type 'int'\n",
    "    learning_rate - the step size for the update step, scalar of type 'float'\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for iteration in range(iterations):\n",
    "        # YOUR CODE HERE\n",
    "        print(f\"Iteration {iteration + 1}/{iterations} | Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "192752d36c595905f57732551b24d03a",
     "grade": false,
     "grade_id": "cell-7967c3cc5a3f7144",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will now apply your newly created `NeuralNetwork`-class on a slightly more difficult problem than before. Given images of hand-drawn numbers, your network will be tasked with predicting the correct digit. For this we will use the famous [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). Below, we have prepared the necessary code needed to load and visualize the data. Run the cells below to see the size of the dataset splits and a visualization of some samples from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb4149ded3ccef9c165818b8c28c98be",
     "grade": false,
     "grade_id": "cell-b3b43b58e7df15ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST data from sklearn\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "x, y = mnist[\"data\"].to_numpy(), mnist[\"target\"].to_numpy()\n",
    "\n",
    "# Normalize the data\n",
    "x = x / 255.0\n",
    "y = y.astype(int)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_one_hot = np.zeros((y.size, y.max() + 1))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train, x_test = x[:60000], x[60000:]\n",
    "y_train, y_test = y_one_hot[:60000], y_one_hot[60000:]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "507b5c7799bbc3bb6fe7c8b750d03823",
     "grade": false,
     "grade_id": "cell-17e84b07d6afd1e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Display the mnist data and labels for 3 samples\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    axs[i].imshow(x_train[i].reshape(28, 28), cmap=\"gray\")\n",
    "    axs[i].set_title(f\"Label: {np.argmax(y_train[i])}\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84188cb008681913d74dbd4418dcf01f",
     "grade": false,
     "grade_id": "cell-2f10ff978191df49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before training the network, we want some way of running predictions on test data and calculate some metric that tells us how our network performed. In this case, we want to calculate the accuracy of our network, meaning the ratio of correctly predicted samples out of the full test set. Implement the function `predict_and_correct`which takes in a `NeuralNetwork`-object and some test data, makes predictions using the neural network, and finally outputs the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f513b997cb08b2eb4839f6b51e084d56",
     "grade": false,
     "grade_id": "cell-6da3ee515976083c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_and_correct(model, x_test, y_test):\n",
    "    \"\"\"Implement a function that predicts the labels of the test data and calculates the accuracy of the predictions.\n",
    "\n",
    "    Arguments:\n",
    "    model - the neural network to use for prediction. Has type `NeuralNetwork`\n",
    "\n",
    "    Returns:\n",
    "    accuracy - the accuracy of the predictions. Has type 'float'\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b825b0306cac95743c9f5bd102d23a22",
     "grade": false,
     "grade_id": "cell-a80a3089babe6e2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you can first test the accuracy of your untrained network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f4dfeb404c92fd46fa715c0ef5a8e2f",
     "grade": false,
     "grade_id": "cell-8be51b057a322bc3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(784, [64, 10])\n",
    "print(\"Accuracy without training: \", predict_and_correct(nn, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f40c0f8bbc87a55277a69d6e5259206a",
     "grade": false,
     "grade_id": "cell-49d2b9c71489f830",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How did it perform? Was it in line with your expectations? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61aaad77c0c37bc61d176063e963d044",
     "grade": true,
     "grade_id": "cell-c3ddb303ba27f764",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95b423e5210a7aefeaf0ba65dee62f25",
     "grade": false,
     "grade_id": "cell-1cf38a3eaa540aaf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's train the network for some iterations and again evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fce7469a8b73b1c1a6c94504cd3ddf73",
     "grade": false,
     "grade_id": "cell-86d816b7880df764",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "train(nn, x_train, y_train, iterations=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f11cf4504b186f4751bace7df893f7d",
     "grade": false,
     "grade_id": "cell-5ac898dad54c3ca4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy after training: \", predict_and_correct(nn, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e720398a40526294d583aaebce979d58",
     "grade": false,
     "grade_id": "cell-9cf20b76fa1bd4c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How did the network perform after training? Was it in line with your expectations? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fe9acf041bbc0572fb1cbb5912b7826",
     "grade": true,
     "grade_id": "cell-56176c004ccc756f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "580a4a9c7c43b18526576b627bb6f420",
     "grade": false,
     "grade_id": "cell-4d4a48b648de562a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below is some code to visualize your networks prediction for a chosen sample in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31e80641d60f3157065b84fdd88027bf",
     "grade": false,
     "grade_id": "cell-1e572ea1dc4d4690",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a function that visualizes the data, prediction and true label for some sample index in the test set\n",
    "def visualize_prediction(model, x_test, y_test, sample_idx):\n",
    "    \"\"\"Implement a function that visualizes the data, prediction and true label for a sample in the test set.\n",
    "\n",
    "    Arguments:\n",
    "    model - the neural network to use for prediction. Has type `NeuralNetwork`\n",
    "    x_test - the input to the neural network, shape (BATCH_SIZE, INPUT_FEATURES) and type 'numpy.ndarray'\n",
    "    y_test - the true value of the output, shape (BATCH_SIZE, OUTPUT_FEATURES) and type 'numpy.ndarray'\n",
    "    sample_idx - the index of the sample to visualize. Has type 'int'\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    assert (sample_idx >= 0) & (\n",
    "        sample_idx < x_test.shape[0]\n",
    "    ), \"Sample index out of bounds\"\n",
    "\n",
    "    y_pred = model.forward_prop(x_test[sample_idx : sample_idx + 1])\n",
    "    plt.imshow(x_test[sample_idx].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(\n",
    "        f\"Prediction: {np.argmax(y_pred)} | True label: {np.argmax(y_test[sample_idx])}\"\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7acd7a1293f661eb3b944b5d620fac80",
     "grade": false,
     "grade_id": "cell-7351c7c7505f3497",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "visualize_prediction(nn, x_test, y_test, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a49e915629a462648817d0ea9d68cb01",
     "grade": false,
     "grade_id": "cell-0de0870e27634efa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Congratulations!\n",
    "You have successfully implemented a neural network from scratch using only NumPy!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
